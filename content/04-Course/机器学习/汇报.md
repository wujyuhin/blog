# 1.introduction

1.在线学习迅速发展，知识追踪是在线学习中的一个新兴研究领域。
2.学生可以通过不同的练习来掌握知识，反过来，能通过学生的学习顺序推断知识状态
3.利用机器学习的序列模型，检测学生的知识状态变化。
4.具体来说，给定学生的历史学习顺序，测量在不同的学习步长的学习状态，并且利用学习状态预测未来表现。
5.若了解学生知识状态后，学生和教师双方都可避免浪费时间在已掌握的知识概念上。
## 知识追踪问题定义和重要性
![[Pasted image 20240101135312.png]]

## 研究现状
现有kt模型大部分分为概率模型、逻辑模型、深度学习方法
- 概率模型有BKT贝叶斯知识追踪模型（隐马尔可夫模型的应用）
- 逻辑模型，基于logistic函数的模型，估计知识状态概率
- DKT模型，使用了RNN,lstm,用隐藏状态表示学生知识状态
- DKVMN动态键值记忆网络
- CKT卷积知识追踪，结合了知识点网络结构的特征
- SAKT自注意力机制知识追踪模型，结合了selfattention的上下文依赖关系
- AKT上下文感知模型，结合了心理测量
现有的方法有贝叶斯知识追踪（BKT）、深度知识追踪（DKT），这些方法认为准确性越高就表示对知识状态估计越好。
## 研究动机
![[Pasted image 20231228204622.png]]
7.一般模型的问题
做了相应知识点的题目，首次做的时候，容易错，但因为是首次做，所以不应该下降这么快，不符合学生学习的认知过程，尽管学生最开始是错了很多，但正因为是开始期间，所以应该还是有知识获取和增长的，不会一直做题越做越差的。
但显然，这样的变化能让以往的模型的带来更高的准确率
8.另一个问题：
现有模型选择关注这道题是否做错，但是从教学实践上来看，观察学生是否在这个题考察的知识点的进步更具知道性，因为在得到最终成绩后再想做改变为时已晚，所以需要在前期捕捉学生是否以能够完成学习目标的速度在学习

1.探索新的kt任务范式
2.提出新的一致性知识追踪模型lpkt
3.模型考虑学习增益的积极影响和学习遗忘的消极影响，还能考虑区分每个学生的个人的个人进步速度。

# 2.模型详解


## 从RNN简介，到LSTM简介，再过渡到LPKT需要引入那些考量

难在这里

## 介绍框架

### 学习过程建模

任务：表示学习过程

学生集合 $S=\{s_1,s_2,...,s_i,...,s_I\}$
习题集合 $E=\{e_1,e_2,...,2_j,...e_J\}$
知识概念集合 $K=\{k_1,k_2,...,k_m,...k_M\}$
题目对应知识点的 $Q$ 矩阵:
$q_{jm}$表示第$j$道题目考察第$m$个知识点
向量 $q_{e_j}=(0,1,...,1,..,0)$ 表示第$j$道题考察了哪些知识点

一般情况下，当一个习题布置给学生时，学生会根据所学知识花一定时间完成，实际上学习过程就是不断重复做题这个行为，而相邻的题目之间也是有间隔时间。

所以一个完整的学习过程如下：
$\{(e_1,at_1,a_1),it_1,(e_2,at_2,a_2),it_2,...,(e_t,at_t,a_t),it_t\}$
其中$\{(e_t,at_t,a_t)\}$表示一个学习单元，$at_t$是作答时间，$it_t$是题目间隔时间

一个基本学习单元嵌入
![[Pasted image 20240101224143.png]]
![[Pasted image 20240101224019.png]]

## 学习测量

测量学习过程中的内在动态的知识掌握情况

与LSTM类似，设置一个隐藏状态$h_{t}\in R^{M*d_k}$
$h_{t-1}$的每一行是每一个知识点的embeding
隐藏状态$h_{t-1}$表示学生在做完$t-1$道题目知识掌握状态

因此特定知识点的掌握情况$\tilde h_t$表示如下，此处点乘表示向量内积，因此h为矩阵，$\tilde h_{t-1}$为向量，所以无论考察了多少知识点都是转为一个向量
![[Pasted image 20240101224134.png]]

学习模块

上一个学习单元，学习间隔，当前学习单元，上一个学习单元学习完后的知识掌握情况进行合并，表示在完成题目后能够得到的学习增益$lg_t$
![[Pasted image 20240101224124.png]]

但并非所有的学习增益都能够转化为学生的知识的增长，因此需要一个门控制对知识的吸收能力，与吸收到的知识点相关的有
上一个学习单元$l_{t-1}$，
当前学习单元$l_t$，
两个单元的间隔时间$it_t$，
以及做完当前题目后学生对相应知识点的掌握状态$\tilde h_{t-1}$

![[Pasted image 20240101224207.png]]
通过题目知识$lg_t$和学习吸收能力$F_t^l$计算学生学习增益，
此处第一个点乘表示对应元素相乘，因此$LG_t$是一个学生对这道题目的增益吸收向量
第二个点乘表示矩阵相乘，$\tilde {LG_t}$为矩阵，第$j$行表示学习第当前题目时的学生的第$j$个知识点的吸收embedding
![[Pasted image 20240101224219.png]]

遗忘模块

同样设置一个门控制知识的遗忘，与遗忘相关的因素有：
目前知识掌握状况$h_t$，
完成当前题目后学生对不同知识点的吸收量$LG_t$
距离上一道题目的时间$it_t$

$\Gamma_t^f$表示对不同知识点的遗忘程度矩阵，第$j$行表示对第$j$个知识点的遗忘embedding
![[Pasted image 20240101224240.png]]

根据学习的积极效应和遗忘效应更新学生对知识点的掌握状态
第一种更新方法：
第一个等式是对特定知识点的学习向量减去特定知识点的遗忘向量得到
![[Pasted image 20240102112237.png]]
![[Pasted image 20240101224246.png]]
第二种更新方法（神经网络的更新知识状态）
![[Pasted image 20240102170736.png]]
![[Pasted image 20240101224254.png]]

![[Pasted image 20240101224306.png]]
**Objective Funcion**
![[Pasted image 20240101224314.png]]
![[Pasted image 20240102101451.png]]
## 代码简介 

# 3.实验

模型训练损失说明
数据集说明
评价指标体系说明
参数说明

## 1）自身对比实验

### （1）不同学习率调整方法

无调整学习率的方法
StepLR的方法
![[Pasted image 20240101194408.png]]
cosineannealingLR
![[Pasted image 20240101194439.png]]
### （2）不同梯度下降方法

adam算法（已使用）
SGD随机梯度下降
SGC带栋梁的跑一边
RMSProp算法也跑一边
放表

opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)

torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
## 2）对比实验
与其他模型对比
直接抄自己或者论文的

### 3）bert预训练
i.介绍bert
![[Pasted image 20240102115426.png]]
![[Pasted image 20240102115534.png]]
ii.bert预训练模型下载
下载了三个bert预训练模型
![[Pasted image 20240102120336.png]]
![[Pasted image 20240102120404.png]]
![[Pasted image 20240102120454.png]]
iii.介绍如何嵌入模型的


iv.实验结果

# 4.总结

问题总结

1.从研究主体的序列规律设计，增强可解释性

2.学习率的调整

3.预训练方法


思考感悟
问题导向