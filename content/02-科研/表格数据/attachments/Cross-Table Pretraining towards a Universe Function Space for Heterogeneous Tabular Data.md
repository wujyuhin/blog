跨数据集预训练从上游数据中学习可重用模式来支持下游任务，在各个领域都取得了显著的成功。
提出了一个跨表预训练的变压器XTFORMER，用于通用的下游表格预测任务。
对XTFORMER进行预训练以建立元函数空间。在这个空间内，嵌入了各种潜在的预测函数，可以通过指定的坐标位置访问。对于所有预训练和下游数据集，我们使用一个简单的校准模块来确定最佳坐标位置，并从定义的空间中提取适合任务的函数。实验表明，在190个预测任务中，交叉表预训练的XTFORMER在137个(72%)任务中同时赢得了XGBoost和Catboost，在144个(76%)和162个(85%)任务中超过了代表性的深度学习模型FT-Transformer和XTab
# 引言
一些研究证明了表内预训练的有效性，但这些方法有单数据的限制，忽略了其他数据的价值，一些研究进一步讨论数据集上的特征重叠，等等，本文关键观点：建立通用函数空间，该空间异构了上游数据集的不同函数在预训练中，在这个空间中有不同的函数使用，对于给定的目标数据集，可以从空间中提取合适函数，利用相应的特征
# 方法
表格数据集之间的异质性表现为特征和目标空间的多样性，具有不同的特征和目标映射函数。==现有的神经网络模型受训练后固定参数的约束，由于其固有的差异性，只能适应一个映射==，难以覆盖多个映射。然而，要将学习到的参数从一个数据集转移到另一个数据集，关键在于利用跨数据集的可重用参数，我们的方法论见解包括==构建一个通用函数空间==，并系统地将所有映射嵌入到该空间中。这使得==不同的映射可以被表示为空间基函数的各种组合==，基函数参数因此被重用。
- 提出了可校准线性层( Calibratable Linear Layer)
	- 在足够大的有限线性函数集合$\Phi=\{\phi_m\}$构成的线性函数空间$F$
	- 任何线性函数$f$表示为下式子，<font  color="red">v是什么？</font>是一个学习向量，在大量数据集投入训练时更新不同线性层的权重
	- ![[Pasted image 20240227153652.png]]![[Pasted image 20240228095625.png]]
	- 训练基本线性层$\Phi$的参数![[Pasted image 20240228095705.png]]
	- ![[Pasted image 20240228105111.png]]
	- 对于任何新的数据集，我们只能校准系数c = [c1，…]。, cM)∈RM获得最后的函数f
	- 目标：在训练的阶段,基本的线性层会学到有意义的函数空间,从而使其更容易找到最后一个函数参数化的系数c。
	- 与全连接的不同：假设一个线性的输入和输出维度层都是d,训练一个基础线性层相当于使用fullyconnected网络优化$d^2$参数。相比之下，校准cm只需要确定M个参数(在我们的实验中M = 4)
	- 一种简单的训练c的方法：Mcal是一个简单的两层多层感知器(MLP)，后跟一个softmax输出。
	- 线性层线性组合还是线性层，区别在于
		- 灵活性：线性层与系数c单独训练
		- 数据效率：看不懂，用上下文v生成M系数从函数空间中得到一个新映射
- 提出了XTFormer结构
	- 作用：弥补线性层缺陷，满足表达多样性和非线性数据集
	- 思路：集成各种非线性组件，激活函数、selfattention，扩展成一个通用的元函数空间。
	- 结构：四层FT-Transformer，将FFN替换为可校准线性层
	- ![[Pasted image 20240228095212.png]]
	- FFN由两个calinear组成，这两个calinear独立
- 预训练和微调、迁移
	- 跨表预训练：在不同的表格数据集上学习建立元函数空间
	- 两阶段微调
		- 任务校准：确定元函数空间中最适合特定数据集的函数
		- 细化：对所有参数进一步微调寻求更优函数

# 训练
![[Pasted image 20240228105847.png]]
- 预训练阶段
	- 大量数据集
	- 特定任务的预测目标
	- 交叉熵损失进行分类、均方误差损失进行回归
	- 每一个epoch是随机选择一个数据集进行训练（因此所有数据集共同构成共享xtformer）
		- 作用在于能够预训练xtformer的所有分量，特别是线性层的基函数
		- 训练Mcal与xtformer融合，便于从元函数空间中识别数据集的最佳函数
- 任务校准阶段
	- 目标是在元函数空间中确定一个最优函数。在不同的数据集上进行预训练后，假设元函数空间是完善的。
	- 在任务校准阶段，对于预训练阶段未见的数据集(任务)，我们冻结XTFORMER的共享部分，并从头开始训练特定于数据集的组件，以使XTFORMER与任务预测函数对齐，如图3所示。
	- 在此阶段，标准化层作为标准配置进行更新，遵循先前的工作(Houlsby等人，2019)。
	- 任务校准阶段的训练目标与预训练阶段的目标一致:交叉熵或MSE损失取决于任务。
	- 在任务校准阶段之后，我们组装共享和数据集特定组件，以生成给定下游任务的熟练模型
- 微调阶段
	- 进一步探索元函数空间外的可能最优的函数
	- ![[Pasted image 20240228103501.png]]

# 数据
此外，从数据库中排除重复的数据集，确保预训练和下游数据集之间没有重叠。在接下来的工作中，我们选择忽略多类数据集(Grinsztajn et al .， 2022)，因为多分类任务可以分解为多个二进制任务，而多类数据集在表格数据集集合中相对较少。我们总共有294个二元分类数据集和366个回归数据集进行预训练，并在190个下游任务上对预训练好的XTFORMER进行了评估。190个下游任务组织在38个数据集上(20个二进制分类数据集和18个回归数据集)，每个数据集在5个设置下使用。在预处理中，我们使用80:20的随机分割将每个数据集分为训练集和测试集。此外，我们进行了随机抽样，以提取20%的训练样本进行验证。这5个设置分别是:(1)使用所有的训练数据和验证数据(T-full)，以及有限的数据场景:(2)200个训练样本包含50个验证数据(T-200)，(3) 100个训练数据包含25个验证数据(T-100)，(4) 50个训练数据包含13个验证数据(T-50)，(5) 20个训练数据包含5个验证数据(T-20)。

# 总结
主要有四个transformer block，里面嵌入了线性函数空间
预训练的时候放入不同的数据，更新其中的线性层基函数
校准阶段理解的是冻结transformer部分，更新v部分？v本质上是选择基函数，也就是选择训练出适合输入表格数据的基函数
然后才是微调