使预训练的语言模型在表格预测方面表现出色
# 摘要
- 深度学习在图像和语言方面有重大进展，就差在表格数据上做了
- 由于深度学习能够理解各种表名称的能力，所以应该是有潜在的跨表传递知识能力
- 但是也不是说做就能做的，因为文本和表数据不兼容
- 提出了模型，==用”相对大小标记法“将标量数值转为离散高维标记，用内在特征关注方法将特征和特征名称集成==？？
# 引言

- XTab则使用特定于数据集的编码器探索了更广泛的领域
- TransTab专注于具有通用特征名称的临床试验表，促进部分重叠的特征嵌入
- tabular prediction adaptive 提出BERT方法
	- 有应对数值特征、也有lm特点
	- 具体是将数字特征里三位相对大小的标记，然后是为有意义的单词
	- 相对大小能在语言空间中感知相对数值大小
- 开发内部特征关注模块IFA？
# 表格预测自适应bert方法
## 用于数值表示的相对量级标记化RMT(Relative Magnitude Tokenization)
- 数值离散化（参考[[决策树]]）
	- ![[Pasted image 20240229095734.png]]
	- 用C4.5离散化数值特征，每个叶子会拆分为多个bin，所以会有多个$e^{i}$
	- 对第$i$个特征的数据进行边界确定$e^{i}$，如果k指的是![[Pasted image 20240229100752.png]]在某个范围内的数值特征对应的bin索引
	- bin的最大数值为256，k的大小是相对大小
- 大小标记
	- 将数值转化为语言空间，所以将上述的bin看作词
	- 所以每个数值都对应一个bin，就成成了一个词
	- 不太懂的是C4.5离散化本身是通过训练得到的吗，其中有很多属性的，难道是就对一个属性作为最优节点，然后用C4.5找到最佳的阈值划分？
	- ![[Pasted image 20240229105859.png]]
- 表格特征预处理
	- 表格会含有不同类型的特征，可以通过连接特证名和值，特征名能embedding，值也呢个embedding，
## IFA
- 在LM处理前用特征名称-值匹配的特征内注意模块IFA
	- ![[Pasted image 20240229212632.png]]
	- Cls 符号一个映射，$E^{(i)}$ 是名称和值的 embedding 的concat
	- 通过一个多头 self-attention，得到一个包含名称和值的信息的 $h_i$
## 预训练范例
输入一个表格，将不同特征的开头 cls、名称、值进行 concat，通过多头子注意力机制得到一个关联输出，这表示每一个特征都有一个输出，一个表格有 n 个特征就有 n 个输出。这 n 个输出是语言模型的输入，为了得到这个表格的下游任务（分类或者回归）
![[Pasted image 20240229210106.png]]
# 训练过程理解
## 数据集
预训练数据和下游数据集不会出现交集
预训练数据集101个二进制分类数据集和101回归数据集
下游数据集是80个二分类数据集65回归数据集
