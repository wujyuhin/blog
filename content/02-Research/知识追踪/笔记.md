lpkt的ednet数据处理
1.对每个student小于1道题目的学生去掉
2.在对user的回答匹配答案的时候，有标答和有知识标记的题目才纳入考虑

文件预处理（ednet_dataset）
1.由question.csv文件生成的关键集合有 
	1.problem2id: 是 q123 -> 自然序号
	2.question2answer: 是  q123 -> 回答abcd
	3.kc2id: 是 12 -> 自然序号
	4.question2kc: 是 q123 ->  kc2id
	5.bundle2id: bundle -> 自然序号
	6.question2bundle: q123 -> bundle2id
2.由所有u*.csv文件集合生成的数据
	【注意：每次录入的csv，都需要提取question2kc和question2answer存在的元素，即具有标答和具有知识点标记的题目才有用】
	1.集合
		1)it2id
		2)at2id
			
文件再处理（prepare_dataset_ednet）
[]
1.导入上述数据后遍历u*.csv数据生成s，p，a，it，at



lpkt跑baseline
1.在dataset prepare中分出了train和test，然后对train分出了五折交叉的train和valid【已完成】
2.对每一份train和valid，可以跑多个epoch，进行模型选择，例如epoch=20
	逐个比较选取最好的模型
		1.记录全部valid_auc，最好的auc对应的model【完成】
3.检查为什么test的auc比valid的高，将train中代码分开
	可能需要重新检查lpkt的模型代码
4.ednet需要重新处理数据！就是不需要拆分，应该比较快

