# 区别
## finetune
这是对预训练模型的基础上多增加与任务相关的输出层，然后利用特定的任务数据对整个模型进行训练调整权重
优点是技能学习到与任务相关的特征，也能有任务数据特征
对于bert来说就好理解，但是gpt来说是如何微调？即gpt的预训练模型是什么样子
bert预训练模型是选词填空
gpt预训练模型是文字接龙-finetune例子对不同句子进行情感分类，那么每一个句子预训练模型，GPT模型通常会返回最后一个时间步的隐藏状态，都有一个隐藏状态，对这个隐藏状态进行激活输出二分类概率预测

**这个“最后一个时间步的隐藏状态”是指当模型处理完所有输入令牌之后，Transformer Decoder部分最后一层的输出。它包含了模型对整个输入序列的理解和上下文信息的综合表示。为了进行文本分类等任务，我们可以在这个隐藏状态之上添加一个额外的全连接层（classification head），并训练该层来基于这个向量表达做出二分类或多分类决策，而不是继续生成新的文本。**
## prompt
用作引导预训练模型产生符合特定任务的输出
在bert中不怎么见，gpt中常见，因为是生成模型

# 如何让模型变成专才
对bert模型可以加外挂，即重新训练应对不同的下游任务
对bert模型可以加插件应对不同任务，不同任务只需要改动adapter
![[Pasted image 20240228204136.png]]

![[Pasted image 20240228204338.png]]

# gpt通才

## in-context learning：给出文本提示回答范例
实际上下图中的任务，gpt本身就已经会做，但是需要告诉他需要做这个任务，再如翻译、摘要等

![[Pasted image 20240228204714.png]]
## instruction-tuni
ng，告诉他做什么是正确的，然后让他做其他事情
chain of thought prompting