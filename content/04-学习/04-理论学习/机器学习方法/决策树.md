
# 信息论
## 信息量
infomation
$$Entropy(S)=-\Sigma^C p_i log_2(p_i)$$
$S$表示样本数，$C$表示样本集合中有多少类，$p_i$表示第$i$个类的概率（直接算频率）
特点：系统越乱信息熵越高
## 信息熵
$$ExpectedEntropy(S_F)=\Sigma_{(v\in V \bigcap F)} p(v) Entropy(S_v)$$
期望信息熵是指对某个属性分类后，取不同属性值下的样本信息熵（男生的信息上，女生的信息熵，然后求期望）
## 信息增益
$$\begin{aligned}
Gain(S,F)=Entropy(S)-ExpectedEntropy(S_F)  \\ 

\end{aligned} $$
计算分类后信息量会降低多少
![[Pasted image 20240229103524.png]]
# 决策树
## C4.5
- 优点：
	- 克服ID3倾向取值较多的属性的缺点
	- 克服ID3只能做离散属性的缺陷
- 
实例数据
![[Pasted image 20240229103752.png]]
- 步骤：
	- 计算总信息量
	- 计算某个属性中不同取值的信息量
	- 计算某个属性的信息熵
	- 计算某个属性的信息增益
	- 计算所有属性的信息增益
	- 信息增益最大的属性作为根节点分类
	- 然后重复上述步骤
![[Pasted image 20240229103804.png]]