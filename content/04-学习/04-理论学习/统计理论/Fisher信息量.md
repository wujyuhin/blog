[由浅入深理解 Fisher 信息（1）——从10种不同的角度和深度理解 Fisher information - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/589273267)
## 1. 参数估计问题

Fisher 信息来源于参数估计问题。首先介绍一下参数估计。
### **1.1 参数估计是做什么？**
定义需要估计的参数为 $\theta$ ，定义观测得到的随机变量为 X 。
这里的观测量X是与参数 $\theta$ 有关系的，或者说 X 包含了与 $\theta$ 有关的信息。
我们通过 n 次观测可以得到n 个观测量： x1,x2,⋯,xn . 我们期望通过这 n 个观测量来估计出最优的参数值 $\hat\theta$ .

**对参数估计的简单解释：**
- 这里的大写 X 是随机变量的意思， 与传统变量不同，它是随机结果映射的数值，受随机性的影响；
- 小写的 xi 是第 i 次观测得到的观测值；
- 观测量 X 是与参数 $\theta$ 有关系的，也就是 X 可以用 $\theta$ 的关系式来表达，例如 X=h($\theta$)+$\mu$ ，其中 h($\theta$) 是与 $\theta$ 有关的一个关系式， $\mu$ 是观测噪声。
### **1.2参数估计的两个问题**
在参数估计中，有两个问题需要解决：
- 问题1：如何用多次观测的观测量来准确估计出参数值？
- 问题2：用多少次观测量才能估计出一个较为准确的参数值？
其中问题1 有很多算法可以解决，但是对于问题2来说，通常是被忽略的。实际上对于估计问题，观测次数越多，估计的参数值也就越准确。但是在实际问题中，通常需要知道如何才能更快速地估计出参数值，这时就需要研究 Fisher 信息来指导我们的观测了。

### **1.3 Fisher信息的理解(1)**
对于问题1，Fisher信息给出了估计参数准确性的最好的界限。参数估计可以有很多方法，例如：矩估计、最大似然估计、最小二乘估计、卡尔曼滤波估计等估计算法来估计参数。但是不管用什么估计方法，其估计结果的准确性都不可能高于Fisher信息定义的最优准确性。

对于问题2，Fisher信息能够指导观测。因为通过计算Fisher信息，可以知道当前观测量所包含的信息量的多少，那么也可以分析出什么情况下的观测能够包含更多的信息量，从而也就分析出了什么是好的观测。

总的来说，Fisher信息的表达式可以预测估计的结果可以有多么准确。甚至在进行观测之前就能预测出这次观测能有多准确，不需要进行仿真实验，不需要知道观测的具体细节。所以可以用来评估观测的回报是否足够大，也就是说可以提前设计如何进行观测。

  

## **2. 两个简单例子：**

我们先用两个简单的例子来举例说明。下面的两个例子中包含全部的3个问题：什么是参数估计、如何估计参数、如何快速估计参数。

### **2.1 例子1**

在一个恒温房间内有一个温度计，可以测量房间的温度。

我们想要估计此房间的温度 $\theta$temp 是多少。房间内温度计是标准温度计，测量时会有一个随机噪声误差。温度计测量得到的温度值定义为随机变量 Xtemp . 由于随机误差的存在，所以测量得到的温度是：

Xtemp=$\theta$temp+$\mu$ ，

其中 $\mu$ 是随机噪声，符合正态分布 $\mu$∼N(0,σ2) .

经过多次观测可以得到 n 个温度值： x1,x2,⋯,xn . 我们想用这 n 个测量的温度 xi 来估计房间中的实际温度 $\theta$temp .

在这个例子中：想要估计的参数是 $\theta$temp ，观测量是 Xtemp ， 观测量与参数之间的关系式是 h($\theta$temp)=$\theta$temp .

### **2.2 例子2**

同样是一个恒温房间内，有一个温度计可以测量房间的温度。但是不同的是，这个温度计由于印刷问题，测量的温度总是偏差一个固定值 $\theta$err 。则有温度计测量到温度为： Xtemp=$\theta$temp+$\theta$err+$\mu$

假设房间的温度 $\theta$temp 是已知的，经过多次观测可以得到 n 个温度值： x1,x2,⋯,xn . 我们想用这 n 个测量的温度 xi 来估计温度计的固定偏差 $\theta$err .

在这个例子中：想要估计的参数是 $\theta$err ，观测量是 Xtemp ， 观测量与参数之间的关系式是 h($\theta$err)=$\theta$temp+$\theta$err .

### **2.3 Fisher 信息的理解(2)**

对于例子1，很直观地对观测量取平均值即可。但是不同的温度计可能需要的观测次数不同。

- 如果是很普通的温度计，仅观测1~2次温度计，其实得到的房间温度是不准确的，也就是噪声 $\mu$ 的方差较大；如果观测100次房间温度，然后取平均值，那么得到的房间温度就基本上是较为准确的了。这个时候我们说观测量包含的参数信息不多。
- 如果我们的温度计是高精度的精密温度计，也就是噪声 $\mu$ 的方差很小，可能仅观测1~2次温度，就可以得到非常准确的房间温度。这时我们说此观测量包含了较多的有关参数的信息。

所以我们得出一个简单的结论：观测噪声的方差越大，观测量所包含的信息越少，反之越多。这就是对 Fisher 信息最简单直观的理解。

##  **3. 最大似然估计**

### **3.1 似然函数**

当进行了 n 次观测后，如何用这 n 次的观测值来估计未知参数时，最大似然估计是最常用的估计方法。

定义随机变量 X 的概率密度函数是 f(x|$\theta$) ，这里的 $\theta$ 是条件概率。例如当 X∼N($\theta$,σ) 时，其概率密度函数为：

f(x|$\theta$)=12πσe−(x−$\theta$)22σ2

似然函数的形式是：

L($\theta$)=∏i=1np(xi|$\theta$)

其中 p(xi|$\theta$) 是参数取 $\theta$ 时，观测值是 xi 的概率大小。似然函数是 $\theta$ 的函数，意义是对 $\theta$ 取值的概率，例如 L($\theta$=2) 的值就是当 $\theta$=2 时的概率大小。

对于连续随机变量，似然函数也可以写成：

L($\theta$)=∏i=1nf(xi|$\theta$)

  

### **3.2 最大似然估计**

似然函数 L($\theta$) 的意义是对 $\theta$ 取值的概率，那么当此概率最大时， $\theta$ 的取值就是对参数的最优估计值。

通常求 L($\theta$) 取最大值时，可以用求导等于零来计算：

  
∂L($\theta$)∂$\theta$=0

为了方便计算，通常会对似然函数加log，然后再求解，最后结果是一样的：

∂ln⁡L($\theta$)∂$\theta$=∑i=1n∂ln⁡f(xi|$\theta$)∂$\theta$=0

  

### **3.3 举例与画图**

例如：需要估计的参数为 $\theta$ ，观测随机变量为 X ，它们的关系是 X=$\theta$+$\mu$ ，其中 $\mu$∼N(0,σ2) 。期望通过 n 次观测得到的观测量 x1,x2,⋯,xn 来估计参数的最优值 $\theta$^ .

下面分 σ=0.5 和 σ=2 分别求解概率密度函数、似然函数、似然函数的导数，并画图分析。我们令参数的真值为 $\theta$0=2 ，观测次数 n=1000 ，来进行画图和计算。

- 概率密度函数： f(x|$\theta$)=12πσe−(x−$\theta$)22σ2
- log似然函数： L($\theta$)=nln⁡12πσ−∑(xi−$\theta$)22σ2
- log似然函数的一阶导： ∑∂ln⁡f(xi|$\theta$)∂$\theta$=∑xi−$\theta$σ2

![[Pasted image 20240818152701.png]]
![[Pasted image 20240818152555.png]]
![[Pasted image 20240818170141.png]]
![[Pasted image 20240818162517.png]]
![](https://pic1.zhimg.com/80/v2-d0e1c3077f0186b3b9a5fe69b85a0754_720w.webp)

三个函数的曲线图

从上图可以看到：

- 概率密度函数中，随机变量 X 的期望是2；
- 在似然函数中， $\theta$ 取2时，概率最大，所以最优参数估计值是 $\theta$^=2 ;
- 在似然函数的一阶导中， $\theta$=2 时，函数值等于0。也就是说，此时似然函数取得最大值。

实际上 L 函数的边际函数一个是似然函数，一个是 x 的密度函数
 ![[Pasted image 20240818160747.png]]

### **3.4 Fisher 信息的理解(3)**

上面的例子中：

- σ=0.5 时，概率密度函数是有一个尖峰的，观测量会集中在尖峰之中。所以仅需要很少的观测，就可以估计出均值和期望，也就是未知参数。这时我们说观测量 X 包含了较多的与未知参数 $\theta$ 有关的信息。
- σ=2 时，概率密度函数很平坦，观测量很分散。所以需要更多的观测，才能估计出未知参数。这时我们说观测量 X 包含的关于 $\theta$ 的信息很少。

所以可以得出一个同样的结论：方差越大，信息量越少。实际上，上面例子的 Fisher 信息等于 I=1/σ2 ，这在后面给出详细的推导和证明。

但是并不能将 Fisher 信息简单定义为 F=1/σ2 ，当噪声不是正态分布时，或者其他不是分析噪声的情况下，观测随机变量 X 的方差并不能完全反应其包含的信息量。我们继续往下分析。


## **4. score 函数的定义**

### **4.1 似然函数的形状分析**

最大似然估计仅考虑似然函数取得最大值的情况，而对似然函数的形状并没有很多的研究，现在思考一下，似然函数的平坦与尖峰是否会对估计结果有影响呢？我们把两种情况的似然函数曲线拿出来单独分析。

![](https://pic2.zhimg.com/80/v2-baa8db6f6ee24115d1e5601b19802851_720w.webp)

似然函数

两种方差情况下的似然函数，都是在 $\theta$=2 时取得最大值，但是

- 当 σ=2 时， $\theta$ 取 2附近的概率也不小（虽然比取2时小）；
- 当 σ=0.5 时， $\theta$ 取 2 附近的值时，概率会快速变小。

那是不是可以说：当 σ=0.5 时，对 $\theta$ 的估计就更稳定，或更容易？（同时也意味着信息量越多）。

而似然函数的形状是否平坦或尖峰，跟其一阶导有关，所以我们定义了score 函数：

S($\theta$)=∂ln⁡L($\theta$)∂$\theta$

### **4.2 Fisher信息的理解(4)**

Score 函数的绝对值大小反映了似然函数的平坦程度，所以：

- 当 score 函数绝对值越大时，也就意味着信息量越多；
- 当 score 函数绝对值越小，信息量就越少。

这里同样不能简单地将 Fisher信息定义为 Score 函数绝对值的大小（虽然在例子的情况下是等效的），因为在其他情况下是不适用的，所以需要一个泛化的定义。

  

## **5. Fisher 信息的定义**

### **5.1 Score函数的绝对值大小分析**

Score 函数是一个变化的函数，对于不同的情况会有不同形状的 Score 函数，单纯对比Score函数的绝对值大小有时候是不正确的，所以需要一个可以适用任何情况下的 Fisher 信息的定义。下面我们单独分析 Score 函数。

![](https://pic3.zhimg.com/80/v2-b6d4df88f4d6b0d8236495fd0bca61c6_720w.webp)

Score 函数

Score函数有一个特点是：在 $\theta$ 取得真值时， S($\theta$0)=0 ，所以 Score 函数是一定会过零点的。而 Score函数的期望同样等于零： E(S($\theta$))=0 。那么 Score 函数的方差是不是就反映了Score函数偏离0点的多少？总结一下：

- Score函数的方差越大，Score函数偏离零点越多，Score函数的整体绝对值越大，log似然函数的形状越尖凸，观测量包含的信息量就越多。
- Score函数的方差越小，Score函数偏离零点越小，Score函数的整体绝对值越小，log似然函数的形状越平坦，观测量包含的信息量就越少。

上面的结论对任何情况下的 score函数都成立，所以可以定义Fisher信息为：

I($\theta$)=Var(S($\theta$))

对上面曲线求方差：

- 当 σ=0.5 时， Var(S($\theta$))=4;
- 当 σ=2 时， Var(S($\theta$))=0.25 .

### **5.2 Fisher 信息的理解(5)**

Fisher信息定义为 Score 函数的方差：

- Score函数的方差越大，Score函数偏离零点越多，Score函数的整体绝对值越大，log似然函数的形状越尖凸，观测量包含的信息量就越多。
- Score函数的方差越小，Score函数偏离零点越小，Score函数的整体绝对值越小，log似然函数的形状越平坦，观测量包含的信息量就越少。

### **5.3 Fisher信息的另外两种计算方法**

进一步化简Fisher信息 I($\theta$)=Var(S($\theta$)) 。

**计算方法1：**

由于 E(S($\theta$))=0 ，所以有

I($\theta$)=Var(S($\theta$))=E(S2($\theta$))−E2(S($\theta$))=E(S2($\theta$))

  

**计算方法2：**

由于 ∂2∂$\theta$2ln⁡L($\theta$)=∂2∂$\theta$2L($\theta$)L($\theta$)−(∂∂$\theta$L($\theta$)L($\theta$))2=∂2∂$\theta$2L($\theta$)L($\theta$)−(∂∂$\theta$ln⁡L($\theta$))2 和

E(∂2∂$\theta$2L($\theta$))=0 ，所以有： −∂2∂$\theta$2ln⁡L($\theta$)=(∂∂$\theta$ln⁡L($\theta$))2

所以有：

I($\theta$)=E(S2($\theta$))=E2(∂∂$\theta$ln⁡L($\theta$))=−E(∂2∂$\theta$2ln⁡L($\theta$))

  

**这两种计算方法是最常用的计算方法。**

  

### **5.4 Fisher 信息的理解(6)**

从 I($\theta$)=−E(∂2∂$\theta$2ln⁡L($\theta$)) 可以看到等号后面是似然函数的二阶导，二阶导意味着曲线的曲率，从似然函数的曲线图可以看出：

- σ=0.5 时，似然函数的曲率很大，对 $\theta$ 的估计更容易，Fisher信息量更多；
- σ=2 时，似然函数的曲率很小，对 $\theta$ 的估计更不稳定，Fisher信息量较少  

## **6. Fisher 信息的计算**

### **6.1 一个观测量、一个参数的一般情况**

定义需要估计的参数为 $\theta$ ，定义观测量的随机变量为 X ，假设进行了 n 次观测，得到了 n 个观测量： x1,x2,⋯,xn ，观测量与估计参数的关系为： X=h($\theta$)+$\mu$ ，其中 $\mu$∼N(0,σ2) 符合正态分布。

则有 X∼N(h($\theta$),σ2) 符合正态分布，则概率密度函数为：

f(x|$\theta$)=12πσ2e−(x−h($\theta$))22σ2

  

log 似然函数为：

ln⁡L($\theta$)=nln⁡12πσ−∑i=1n(xi−h($\theta$))22σ2

  

Score 函数为：

S($\theta$)=∂ln⁡L($\theta$)∂$\theta$=∑i=1n(xi−h($\theta$))h′($\theta$)σ2

  

Fisher 信息为：

I($\theta$)=−E(∑i=1n∂2ln⁡L($\theta$)∂$\theta$2)=1σ2E[∑i=1n(h′($\theta$))2+∑i=1nh″($\theta$)(xi−h($\theta$))]=∑i=1n(h′($\theta$))2σ2+1σ2E[∑i=1nh″($\theta$)(xi−h($\theta$))]=∑i=1n(h′($\theta$))2σ2

可以看到Fisher信息的计算主要是 h($\theta$) 的导数。

### **6.2 Fisher 信息的理解(7)**

可以看到最后的Fisher信息的计算有一个求和，这就是它非常神奇（厉害）的地方。这个求和意味着：观测次数越多，总的信息量也就越多！Fisher信息就是把每次观测得到的信息量加到一起来求和。

  

### **6.3 多个观测量、多个参数的一般情况**

对于多个观测量和多个未知参数的情况下，Fisher信息的表达是一个矩阵的形式。本文不再详细推导矩阵形式的Fisher信息的表达式，而是直接给出结果和计算过程。

假设有 m 个观测量： X1,X2,⋯,Xm ；每次观测都可以得到 m 个观测量，例如第k次观测可以得到： X1(k),X2(k),⋯,Xm(k) ；总共进行了 n 次观测，总共得到了 m×n 个观测量。假设有 r 个未知参数： $\theta$1,$\theta$2,⋯,$\theta$r .

则 Fisher 信息矩阵是一个 r×r 维的对称方阵：

Iij=∑k=1n∑z=1m1σz2∂xz∂$\theta$i∂xz∂$\theta$j

其中 σz2 是观测量 Xm 的观测噪声的方差。

如果观测量的噪声方差是相同的，则可以使用矩阵求导的形式来计算：

I=1σ2∑k=1n(dxd$\theta$T)T(dxd$\theta$T)

  

### **6.4 Fisher 信息的理解(8)**

从Fisher信息的定义、单参数的求解过程、多参数的求解过程可以看出，Fisher信息的计算仅需要观测方程和观测噪声的方差。

## **7. Cramer-Rao 下界**

### **7.1 估计参数的方差**

给定了多次观测的观测值，用这些值来估计未知参数。虽然使用最大似然估计等估计算法可以计算出最优的参数估计值，但是此最优估计值与真值之间还是存在偏差的。对于一个估计器的衡量，主要有三个方面：

- 无偏性， E(θ^)=θ0 ，即估计参数的期望等于参数真值；
- 有效性， Var(θ^) ，即估计参数的方差，刻画对估计值的信任程度，方差越小越好；
- 一致性， limn→∞p{‖θ^−θ0‖<ϵ}=1 ，即当样本数量逐渐增加时，估计量逐渐收敛于真值。

Cramer-Rao 下界指的是参数估计值方差的最小值是多少，其计算公式为：

Var(θ^)≥1I(θ)

  

### **7.2 Fisher 信息的理解(9)**

从 Cramer-Rao 下界的定义可以看出：任何无偏估计的方差至少大于Fisher信息的倒数。也就是Fisher信息定义了参数估计值最优能够达到的方差精度。

  

## **8. Fisher 信息的应用举例**

### **8.1 作为目标函数来优化**

将Fisher信息作为目标函数来优化观测，通常可以得到优质的观测量。

Fisher信息矩阵中的每个元素都表示的是信息量的多少，但是如果将Fisher信息作为目标函数进行优化时，需要的是一个标量的数值，此时通常情况下会选择Fisher信息矩阵的行列式作为目标函数：

J=‖I‖

  

这是一种降维的操作，虽然损失了一些内涵的具体信息，但是也能反应整体的信息量的多少。

其他常用的降维方法有：

- J=−ln⁡det{I} ;
- J=maxieig(I−1) ;
- J=Tr{I−1} ;
- J=−Tr{I} .

### **8.2 举例：鸡兔同笼问题**

鸡兔同笼问题非常简单，是通过腿和头的数量来计算鸡和兔的数量。

但是如果笼子里的鸡和兔特别多，我们数腿和头的数量的时候有可能数错，那么我们是数腿的时候认真一些好呢，还是数头的时候认真一些好呢？

**问题的数学描述：**

定义鸡的数量为 θch ，兔的数量为 θra ；定义观测的腿的数量为 xleg ，头的数量为 xhead ;观测的腿的数量和头的数量都存在观测误差，观测误差符合正态分布。则有

xleg=2θch+4θra+μlegxhead=θch+θra+μhead

  

其中 μleg∼N(0,σleg2) ， μhead∼N(0,σhead2) .

最后问题变为 μleg 和 μhead 哪个对最后的估计结果影响更大？

**问题求解：**

假如仅进行了一次观测，求Fisher信息矩阵

I=[4σleg2+1σhead28σleg2+1σhead28σleg2+1σhead216σleg2+1σhead2]

  

Fisher信息矩阵的迹为： Tr(I)=20σleg2+1σhead2 .

很明显， σleg2 的减小对 Tr(I) 的增加贡献更大，所以在数腿的时候更认真一些，最后的估计结果会更好。

  

### **8.3 bearing-only 目标估计问题**

**背景介绍：**

在海洋中的潜艇会通过声呐来探测其他潜艇的位置，通常为了不被其他潜艇发现，都使用被动声呐来探测，也就是只接收声呐信号，不发射声呐信号。此时只能探测得到其他潜艇的方向信息（bearing），并不知道距离信息。

**问题描述：**

假设目标的位置 pT 是固定不变的，观测者的位置为 po(t) 是已知的且在不断变化中，每次观测可以得到方向信息 g^(t) 是单位方向向量，所以有 ‖g^(t)‖=1 。

- 问题1：期望通过n次观测得到的 g^(t0),g^(t1),⋯,g^(tn) 来估计出目标的位置 pT ；
- 问题2：希望知道什么样的观测，或者说观测者在什么位置时，可以更好地估计目标的位置？

**问题的解决：**

对于问题1，使用最大似然估计、最小二乘、卡尔曼滤波等估计算法都可以做到估计目标的位置，本文不再详细推导。

对于问题2，可以使用Fisher信息矩阵来解决。假设观测噪声的方差相同，首先根据观测量和位置参数的关系式来求Fisher信息矩阵，有观测量的定义可以得到：

g(t)=pT−po(t)‖pT−po(t)‖g^(t)=pT−po(t)‖pT−po(t)‖+μ(t)  
  

其中 g(t) 是真实的方向信息， μ(t) 是符合正态分布的观测噪声。

同时有Fisher 信息矩阵为：  
I=∑i=t1tn(dg(i)dpTT)Σ−1(dg(i)dpTT)T=Σ−1∑i=t1tn[1r2(i)(I−g^(i)g^T(i))]

  

其中 r(t) 为两个潜艇的距离， Σ 为方差矩阵。

将此Fisher信息矩阵的行列式作为目标函数，来优化 po 使得目标函数最大。

maxpoJ=‖I‖

  

在这里我们不再继续往下进行详细的推导，仅从公式上得出一些结论：

- 目标函数中有 1/r(t) ，所以两个潜艇的距离越近越好；
- ‖I−g1g1T+I−g2g2T‖ 的值，随着 g1⋅g2 的增大而增大，所以观测的方向变化越大越好。

这两个结论在现实中考虑也是很好理解的，从两个不同的角度去观测目标，得到两个方向，两个方向相交的点就是目标点。如果观测的两个方向基本相同，则很难估计出目标的位置。

### **8.4 Fisher 信息的理解(10)**

上面的应用举例属于单次不可观问题，也就是仅通过一次观测无法估计出未知参数。

从 6.2节，我们已经知道了观测量越多，得到的信息量也就越多。那么如果仅有一次观测时，信息量会有多小呢？

我们将上面的例子，仅有一次观测量的Fisher信息矩阵写出来：

I=Σ−11r2(t)[I−g(t)gT(t)]  

求行列式可得： ‖I‖=0;

求Fisher信息矩阵的迹可得： Tr(I)=0 .

又一次神奇（厉害）的地方出现了：当观测量无法复原（估计）出未知参数时，其Fisher信息矩阵的行列式等于零。

我们继续验证，当两次观测的方向是正好相反时，也就是 g(t1)=−g(t2) ，此时是两次观测，但是同样无法估计未知参数，此时的Fisher信息矩阵是：

I=Σ−1[1r(t1)[I−g(t1)gT(t1)]+1r(t2)[I−g(t2)gT(t2)]]=Σ−1[1r(t1)+1r(t2)][I−g(t1)gT(t1)]  

同样其行列式为零： ‖I‖=0 ；其迹 Tr(I)=0.



Fisher 信息被定义为似然关于 $\theta$ 变化率的方差
Fisher 信息越大，说明分数绝对值较大，
fisher 信息是观测所有 x 平均拥有多少 $\theta$ 的信息，因此对 $S(\theta)$ 的方法是求所有 x 距离 0 的偏差平均有多少

给定密度函数 $f(x;\theta)$，如果 $f$ 关于 $\theta$ 变化快速达到峰值，说明从数据中容易指导"正确"的 $\theta$，换句话说是，数据 $X$ 提供了参数 $\theta$ 的大量信息
如果 f 是平坦或者分散的，则他需要和大量样本估计 theta

Fisher 信息的第二种表示方法（如果似然函数存在二阶导）
- 数学概念：似然函数是凹函数（有最大值），其二阶导为负数(曲率)，负得越大说明最大值那一点越尖锐，
- 统计角度：若密度函数 $f(x;\theta)$ 越"窄"，使得 $f$ 关于 $\theta$ 变化快速达到峰值，说明该随机变量生成的样本更容易估计精确的 $\theta$ 。为什么？
- 如图说明，给定 $n$ 样本 $x_1,x_2,...,x_n$，对数似然函数关于 $\theta$ 较快到峰值，从信息熵角度看，$n$ 个样本的似然函数如果对不同 $\theta$ 取值的似然值都相差不大，说明这些样本具有真实 $\theta$ 的信息量很少，反而如果变化很大，且不同 $\theta$ 差别很大，说明这些样本具有信息量较高。因此：密度函数越“窄”，本质上是其最大值点越"尖锐"，即二阶导负得越大。
- ![[Pasted image 20240818190057.png]]
- 由于 Fisher 信息不是特定观测值的函数，因此需要对该随机变量平均化。则可写成 $I(\theta)=-E_x(\nabla_{\theta}^2 f(x;\theta))$ ，二阶导负得越大，则信息量 $I(\theta)$ 越大。

第一种表示方法是一阶导的方差（以真实参数 $\theta$ 为条件）
![[Pasted image 20240818183859.png]]
- 统计角度：若密度函数 $f(x;\theta)$ 越"窄"，即随机变量 $X$ 方差越小，使得 $f$ 关于 $\theta$ 变化快速达到峰值，说明该随机变量生成的样本更容易估计精确的 $\theta$ 。
- 数学概念：似然函数有最大值，一阶导变化幅度由负到正，幅度越大说明密度函数变化越快，即函数越"窄"
- 如图说明，给定一个样本 $x$，密度函数越窄，即越陡峭，本质是一阶导的绝对值越大。
- ![[Pasted image 20240818181836.png]]
- 按一阶导方差的定义来看，高 fisher 信息量意味着随机变量的一阶导绝对值通常很高。
- 即大多数样本的绝对值都很高，单个样本使得一阶导